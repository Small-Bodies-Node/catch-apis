# CATCH-APIs v2.0.0

The Planetary Data System Small Bodies Node survey data search tool: catch comets and asteroids in wide-field sky survey data.

## Overview

CATCH-APIs provide a REST API service that enable a user to search for potential observations of comets and asteroids in wide-field sky survey data. This API is designed for use by the Planetary Data System Small Bodies Node (SBN) at the University of Maryland, but it is possible to deploy anywhere with other data sets. SBN is the primary archive for the Near-Earth Asteroid Tracking (NEAT) survey, the Asteroid Terrestrial-impact Last Alert System (ATLAS), the Catalina Sky Survey, and Spacewatch. CATCH-APIs is one of the primary methods for users to discover scientifically interesting data in those data sets.

The API is backed by:

- The [catch](https://github.com/Small-Bodies-Node/catch) and [sbsearch](https://github.com/Small-Bodies-Node/sbsearch) Python libraries, which define how survey metadata is stored and execute the actual searches on the database.
  - The [s2geometry](http://s2geometry.io/) C++ library is used for spatial indexing on the Celestial Sphere.
  - Target ephemerides are generated by [Horizons](https://ssd.jpl.nasa.gov/horizons/) at NASA JPL via [astroquery](https://astroquery.readthedocs.io/).
  - [SQLAlchemy](https://www.sqlalchemy.org/) and [PostgreSQL](https://www.postgresql.org/) store user queries and survey metadata.
  - Metadata ingestion uses the [pds3](https://github.com/mkelley/pds3) or [pds4_tools](https://github.com/Small-Bodies-Node/pds4_tools) Python libraries.
- [flask](https://flask.palletsprojects.com/), [connexion](https://connexion.readthedocs.io/), [openapi](https://swagger.io/specification/), and [gunicorn](https://gunicorn.org/) for the API experience and documentation.
- [redis](https://redis.io/) for the job queue and user task messaging.
- [rq](https://python-rq.org/) and [pm2](https://pm2.keymetrics.io/) execute the job queue.

## Features

- A single observation table holds all observations from all surveys. This feature allows for an approximate search that can ignore parallax and search all surveys in one go.  Fine for objects a few au a way or more.
- The S2 library cells are set to a minimum size of 3e-4 radians (about 1 arcmin), and maximum size of 10 deg.
- Ephemerides may create loops on the sky (e.g., during retrograde motion), this is correctly handled by S2, even when padding the ephemeris with some uncertainty.
- Ephemeris segments are combined into groups 10 degrees or 30 days long (whichever limit is reached first; 10 deg / 30 days = 50" / hour), and the database is queried for the combined line all at once.  This improves performance because the segments are spatially correlated.

## Code Terminology

- worker vs woRQer
  - There are two kinds of worker in this code: gunicorn production workers and redis 'rq' workers. TO distinguish between the two, we call the latter "woRQer" wherever possible
- Service vs Service
  - There are two kinds of service: flask-business logic services and docker services; hopefully it's clear from context which is which

## Setup

CATCH-APIs are developed and run using docker. To develop locally:

- Install `docker` and `docker-compose` on your unix machine
- Clone this repo
- Copy .env-template to .env and edit
- If you are developing in VSCode, then `source _vscode_setup` to install packages on your machine, etc. in order to get intellisense, etc.; the repo comes with a .vscode dir for settings
- Before you start running the docker containers, you will need to obtain a file generated by the `pg_dump` program. Please contact [MKelly](https://github.com/mkelley) or [D-W-D](https://github.com/d-w-d) for such a file. Copy that file within your clone of this repo to `pg-ini-data/some-name.backup`

- Run docker-compose:

  - There are two ways to start all of the relevant containers: dev mode and prod mode. In development mode, the code base for the apis will be "bind-mounted" into the container so that changes made to the code get reflected instantly in the running application (the API and woRQer processes are run via nodemon in dev mode). In prod mode, the code-base is simply copied over into the image, so it will not be picked up dynamically at run-time
    - To run everything in development mode, run `docker-compose -f docker-compose.yml up --build`
      - To stop everything, enter CTRL+C once to stop processes gracefully; sometimes this might fail to properly shutdown everything, in which case you can swap `up --build` with `down` to re-try shutting everything down
      - Also, when you bring docker-compose systems up/down, it's sometimes helpful to also remove the stopped containers with `docker container prune`
    - To run everything in prod mode, run `docker-compose -f docker-compose.prod.yml up --build`

- DB setup:
  - The CATCH tool requires a postgresDB populated with initial data. This repo runs a postgres container. Before you can run any catch queries through the apis, you need to initialize the postgres db by jumping into the db container (the `_docker_enterer` script makes this easy), and going to `/docker-entrypoint-initdb.d`. Now you can source the script `_restore.sh` and provide the name of the backup file; this script will run `pg_restore` on that file.
  - The restoration of the pgdb can take a while; on a high-end Mac it took ~10 mins to restore a backup file of 250MB, with a resultant db of size 3.4gb.

## Setup (Deprecated)

CATCH-APIs are developed for linux/macos systems. Python requirements are detailed in [setup.cfg](setup.cfg).

1. Create the database and setup user access:
   1. The user account that maintains the database (e.g., adds new observations), requires CREATE privileges on the database, and SELECT and INSERT privileges on all tables.
   2. The user account that owns the API instances will need SELECT privileges on all tables, and INSERT privileges on the obj, designation, catch_query, caught, and found tables.
2. Copy `.env-template` to `.env` and edit according to the comments.
3. The `_initial_setup` script builds a virtual environment with all required libraries. Using `bash`, source this file to run the script and load the new environment: `source _initial_setup`.
4. Insert data into the database. See the [catch README](https://github.com/Small-Bodies-Node/catch) for details.

## Usage

### Local instances for testing

1. Set the `DEPLOYMENT_TIER` in `.env` to `LOCAL`.
2. Open two terminal instances. For each instance, enter the CATCH-APIs virtual environment: `source _activate`.
3. In the first terminal:
   1. Is redis running? `bash _redis_manager status`. If not, start up the redis queuing system: `bash _redis_manager start`.
   2. Start up worker processes: `bash _develop_workers`.
4. In the second terminal, start up the API server: `bash _develop_apis`.
5. Open the swagger documentation. If the `BASE_URL` in `.env` is empty: http://localhost:5003/ui .

### Production

TBD

### Misc Notes

- @MSK: I am not satisfied with the way sbsearch, catch and sbpy are being dockerized right now; need to discuss how they work; why do they always get installed to `src`? I tried simplifying the way/location for their installation but that seemed to break the S2 stuff with sbsearch
